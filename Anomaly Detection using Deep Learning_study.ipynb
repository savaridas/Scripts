{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLog.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtpywv+lhEeJ+zA7kPRR73",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/savaridas/Scripts/blob/master/Anomaly%20Detection%20using%20Deep%20Learning_study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZmkr7ny-mjd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "912cca26-dbcd-4ad5-b51b-1a9d0f196516"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "from spellpy import spell\n",
        "\n",
        "\n",
        "def deeplog_df_transfer(df, event_id_map):\n",
        "    df['datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
        "    df = df[['datetime', 'EventId']]\n",
        "    df['EventId'] = df['EventId'].apply(lambda e: event_id_map[e] if event_id_map.get(e) else -1)\n",
        "    deeplog_df = df.set_index('datetime').resample('1min').apply(_custom_resampler).reset_index()\n",
        "    # deeplog_df['EventId'] = deeplog_df['EventId'].apply(lambda x: list(filter(lambda e: e != -1, x)))\n",
        "    return deeplog_df\n",
        "\n",
        "\n",
        "def _custom_resampler(array_like):\n",
        "    return list(array_like)\n",
        "\n",
        "\n",
        "def deeplog_file_generator(filename, df):\n",
        "    with open(filename, 'w') as f:\n",
        "        for event_id_list in df['EventId']:\n",
        "            for event_id in event_id_list:\n",
        "                f.write(str(event_id) + ' ')\n",
        "            f.write('\\n')\n",
        "\n",
        "\n",
        "if __name__ == '__main__': \n",
        "    ##########\n",
        "    # Parser #\n",
        "    ##########\n",
        "    input_dir = './'\n",
        "    output_dir = './OUTPUT'\n",
        "    log_format = '<Logrecord> <Date> <Time> <Pid> <Level> <Component> \\[<ADDR>\\] <Content>'\n",
        "    log_main = 'open_stack'\n",
        "    tau = 0.5\n",
        "\n",
        "    parser = spell.LogParser(\n",
        "        indir=input_dir,\n",
        "        outdir=output_dir,\n",
        "        log_format=log_format,\n",
        "        logmain=log_main,\n",
        "        tau=tau,\n",
        "    )\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for log_name in glob(input_dir + '*.log'):\n",
        "        print('INPUT_DIR:', input_dir)\n",
        "        log_name = os.path.basename(log_name)\n",
        "        print('LOG_NAME:', log_name)\n",
        "        parser.parse(log_name)\n",
        "\n",
        "   ##################\n",
        "    # Transformation #\n",
        "    ##################\n",
        "    df = pd.read_csv(output_dir + '/openstack_normal1.log_structured.csv')\n",
        "    df_normal = pd.read_csv(output_dir+ '/openstack_normal2.log_structured.csv')\n",
        "    df_abnormal = pd.read_csv(output_dir+ '/openstack_abnormal.log_structured.csv')\n",
        "\n",
        "    event_id_map = dict()\n",
        "    for i, event_id in enumerate(df['EventId'].unique(), 1):\n",
        "        event_id_map[event_id] = i\n",
        "\n",
        "    print('event_id_map:', len(event_id_map))\n",
        "\n",
        "    #########\n",
        "    # Train #\n",
        "    #########\n",
        "    deeplog_train = deeplog_df_transfer(df, event_id_map)\n",
        "    deeplog_file_generator('train', deeplog_train)\n",
        "\n",
        "    ###############\n",
        "    # Test Normal #\n",
        "    ###############\n",
        "    deeplog_test_normal = deeplog_df_transfer(df_normal, event_id_map)\n",
        "    deeplog_file_generator('test_normal', deeplog_test_normal)\n",
        "\n",
        "    #################\n",
        "    # Test Abnormal #\n",
        "    #################\n",
        "    deeplog_test_abnormal = deeplog_df_transfer(df_abnormal, event_id_map)\n",
        "    deeplog_file_generator('test_abnormal', deeplog_test_abnormal)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2020-06-24 20:21:43,874][INFO]: Parsing file: ./openstack_normal1.log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INPUT_DIR: ./\n",
            "LOG_NAME: openstack_normal1.log\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[2020-06-24 20:21:44,163][INFO]: Loaded 19.1% of log lines.\n",
            "[2020-06-24 20:21:44,379][INFO]: Loaded 38.2% of log lines.\n",
            "[2020-06-24 20:21:44,696][INFO]: Loaded 57.3% of log lines.\n",
            "[2020-06-24 20:21:44,926][INFO]: Loaded 76.5% of log lines.\n",
            "[2020-06-24 20:21:45,244][INFO]: Loaded 95.6% of log lines.\n",
            "[2020-06-24 20:21:45,384][INFO]: load_data() finished!\n",
            "[2020-06-24 20:21:45,429][INFO]: Load objects done, lastestLineId: 155347\n",
            "[2020-06-24 20:21:50,258][INFO]: Processed 19.1% of log lines.\n",
            "[2020-06-24 20:21:55,306][INFO]: Processed 38.2% of log lines.\n",
            "[2020-06-24 20:22:00,568][INFO]: Processed 57.4% of log lines.\n",
            "[2020-06-24 20:22:06,131][INFO]: Processed 76.5% of log lines.\n",
            "[2020-06-24 20:22:12,190][INFO]: Processed 95.6% of log lines.\n",
            "[2020-06-24 20:22:13,660][INFO]: Processed 100.0% of log lines.\n",
            "[2020-06-24 20:22:18,051][INFO]: Output parse file\n",
            "[2020-06-24 20:22:19,828][INFO]: lastestLindId: 155347\n",
            "[2020-06-24 20:22:27,737][INFO]: rootNodePath: ./OUTPUT/rootNode.pkl\n",
            "[2020-06-24 20:22:27,787][INFO]: logCluLPath: ./OUTPUT/logCluL.pkl\n",
            "[2020-06-24 20:22:27,815][INFO]: Store objects done.\n",
            "[2020-06-24 20:22:27,816][INFO]: Parsing done. [Time taken: 0:00:43.942699]\n",
            "[2020-06-24 20:22:27,845][INFO]: Parsing file: ./openstack_normal2.log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INPUT_DIR: ./\n",
            "LOG_NAME: openstack_normal2.log\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[2020-06-24 20:22:28,322][INFO]: Loaded 7.3% of log lines.\n",
            "[2020-06-24 20:22:29,015][INFO]: Loaded 14.6% of log lines.\n",
            "[2020-06-24 20:22:29,433][INFO]: Loaded 21.9% of log lines.\n",
            "[2020-06-24 20:22:29,963][INFO]: Loaded 29.2% of log lines.\n",
            "[2020-06-24 20:22:30,417][INFO]: Loaded 36.5% of log lines.\n",
            "[2020-06-24 20:22:30,843][INFO]: Loaded 43.8% of log lines.\n",
            "[2020-06-24 20:22:31,124][INFO]: Loaded 51.1% of log lines.\n",
            "[2020-06-24 20:22:31,507][INFO]: Loaded 58.4% of log lines.\n",
            "[2020-06-24 20:22:31,913][INFO]: Loaded 65.7% of log lines.\n",
            "[2020-06-24 20:22:32,183][INFO]: Loaded 73.0% of log lines.\n",
            "[2020-06-24 20:22:32,442][INFO]: Loaded 80.2% of log lines.\n",
            "[2020-06-24 20:22:32,869][INFO]: Loaded 87.5% of log lines.\n",
            "[2020-06-24 20:22:33,134][INFO]: Loaded 94.8% of log lines.\n",
            "[2020-06-24 20:22:33,634][INFO]: load_data() finished!\n",
            "[2020-06-24 20:22:34,072][INFO]: Load objects done, lastestLineId: 207636\n",
            "[2020-06-24 20:22:36,508][INFO]: Processed 7.3% of log lines.\n",
            "[2020-06-24 20:22:38,725][INFO]: Processed 14.6% of log lines.\n",
            "[2020-06-24 20:22:41,027][INFO]: Processed 21.9% of log lines.\n",
            "[2020-06-24 20:22:43,309][INFO]: Processed 29.2% of log lines.\n",
            "[2020-06-24 20:22:45,612][INFO]: Processed 36.5% of log lines.\n",
            "[2020-06-24 20:22:47,993][INFO]: Processed 43.8% of log lines.\n",
            "[2020-06-24 20:22:50,618][INFO]: Processed 51.1% of log lines.\n",
            "[2020-06-24 20:22:53,455][INFO]: Processed 58.4% of log lines.\n",
            "[2020-06-24 20:22:56,442][INFO]: Processed 65.7% of log lines.\n",
            "[2020-06-24 20:22:59,569][INFO]: Processed 73.0% of log lines.\n",
            "[2020-06-24 20:23:02,812][INFO]: Processed 80.3% of log lines.\n",
            "[2020-06-24 20:23:06,407][INFO]: Processed 87.6% of log lines.\n",
            "[2020-06-24 20:23:08,605][INFO]: Processed 95.0% of log lines.\n",
            "[2020-06-24 20:23:09,718][INFO]: Processed 100.0% of log lines.\n",
            "[2020-06-24 20:23:21,055][INFO]: Output parse file\n",
            "[2020-06-24 20:23:24,267][INFO]: lastestLindId: 207636\n",
            "[2020-06-24 20:23:40,962][INFO]: rootNodePath: ./OUTPUT/rootNode.pkl\n",
            "[2020-06-24 20:23:41,011][INFO]: logCluLPath: ./OUTPUT/logCluL.pkl\n",
            "[2020-06-24 20:23:41,049][INFO]: Store objects done.\n",
            "[2020-06-24 20:23:41,050][INFO]: Parsing done. [Time taken: 0:01:13.204998]\n",
            "[2020-06-24 20:23:41,087][INFO]: Parsing file: ./openstack_abnormal.log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INPUT_DIR: ./\n",
            "LOG_NAME: openstack_abnormal.log\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[2020-06-24 20:23:41,363][INFO]: Loaded 54.2% of log lines.\n",
            "[2020-06-24 20:23:41,560][INFO]: Loaded 100.0% of log lines.\n",
            "[2020-06-24 20:23:41,653][INFO]: load_data() finished!\n",
            "[2020-06-24 20:23:41,845][INFO]: Load objects done, lastestLineId: 344549\n",
            "[2020-06-24 20:23:45,466][INFO]: Processed 54.2% of log lines.\n",
            "[2020-06-24 20:23:48,536][INFO]: Processed 100.0% of log lines.\n",
            "[2020-06-24 20:23:50,138][INFO]: Output parse file\n",
            "[2020-06-24 20:23:52,499][INFO]: lastestLindId: 344549\n",
            "[2020-06-24 20:23:58,833][INFO]: rootNodePath: ./OUTPUT/rootNode.pkl\n",
            "[2020-06-24 20:23:58,877][INFO]: logCluLPath: ./OUTPUT/logCluL.pkl\n",
            "[2020-06-24 20:23:58,907][INFO]: Store objects done.\n",
            "[2020-06-24 20:23:58,909][INFO]: Parsing done. [Time taken: 0:00:17.822048]\n",
            "[2020-06-24 20:24:00,363][INFO]: NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "event_id_map: 1142\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB3pDGdX0Cdl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "97d6326e-8d03-4b3a-809e-ea7606a2ae5c"
      },
      "source": [
        "!pip install spellpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spellpy\n",
            "  Downloading https://files.pythonhosted.org/packages/c8/8a/f9962aefb51b251cfe42e07d2bebf7cd639acb4e35ebbe26990f5536cf0f/spellpy-0.0.9-py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from spellpy) (1.0.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->spellpy) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->spellpy) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->spellpy) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->spellpy) (1.12.0)\n",
            "Installing collected packages: spellpy\n",
            "Successfully installed spellpy-0.0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31m12vROWAqo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "59c9a912-9540-4e58-de47-ec73e670281b"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import logging\n",
        "import argparse\n",
        "\n",
        "import boto3\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG,\n",
        "                    format='[%(asctime)s][%(levelname)s]: %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input):\n",
        "        h0 = torch.zeros(self.num_layers, input.size(0), self.hidden_size)\n",
        "        c0 = torch.zeros(self.num_layers, input.size(0), self.hidden_size)\n",
        "        out, _ = self.lstm(input, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "\n",
        "class Generate():\n",
        "    def __init__(self):\n",
        "        self.init_obj = None\n",
        "\n",
        "    def generate(self, name, window_size, local):\n",
        "        num_sessions = 0\n",
        "        inputs = []\n",
        "        outputs = []\n",
        "\n",
        "        line = self.init_line(local, name)\n",
        "        while line:\n",
        "            line = tuple(map(lambda n: n - 1, map(int, line.strip().split())))\n",
        "            for i in range(len(line) - window_size):\n",
        "                inputs.append(line[i:i+window_size])\n",
        "                outputs.append(line[i+window_size])\n",
        "            line = self.readline(local)\n",
        "            num_sessions += 1\n",
        "        logger.info('Number of session({}): {}'.format(name, len(inputs)))\n",
        "        logger.info('Number of seqs({}): {}'.format(name, len(inputs)))\n",
        "        dataset = TensorDataset(torch.tensor(inputs, dtype=torch.float), torch.tensor(outputs))\n",
        "        return dataset\n",
        "\n",
        "    def init_line(self, local, name):\n",
        "        if local:\n",
        "            f = open(name, 'r')\n",
        "            self.init_obj = f\n",
        "            line = self.init_obj.readline()\n",
        "        else:\n",
        "            client = boto3.client('s3')\n",
        "            bucket = '$YOUR_S3_BUCKET'\n",
        "            prefix = '$YOUR_S3_FOLDER_NAME'\n",
        "            self.init_obj = client.get_object(Bucket=bucket, Key=prefix + name)\n",
        "            line = self.init_obj.get('Body')._raw_stream.readline()\n",
        "            line = line.decode().rstrip()  # decode from byte to string & right strip \\n\n",
        "        return line\n",
        "\n",
        "    def readline(self, local):\n",
        "        if local:\n",
        "            line = self.init_obj.readline()\n",
        "        else:\n",
        "            line = self.init_obj.get('Body')._raw_stream.readline()\n",
        "            line = line.decode().rstrip()\n",
        "        return line\n",
        "\n",
        "\n",
        "def _get_train_data_loader(batch_size, is_distributed, window_size, local, **kwargs):\n",
        "    logger.info(\"Get train data loader\")\n",
        "    _generate = Generate()\n",
        "    seq_dataset = _generate.generate(name='train', window_size=window_size, local=local)\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(seq_dataset) if is_distributed else None\n",
        "    dataloader = DataLoader(seq_dataset, batch_size=batch_size, shuffle=train_sampler is None,\n",
        "                            sampler=train_sampler, **kwargs)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def _average_gradients(model):\n",
        "    # Gradient averaging\n",
        "    size = float(dist.get_world_size())\n",
        "    for param in model.parameters():\n",
        "        dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)\n",
        "        param.grad.data /= size\n",
        "\n",
        "\n",
        "def train(args):\n",
        "    is_distributed = len(args.hosts) > 1 and args.backend is not None\n",
        "    logger.debug(\"Distributed training - {}\".format(is_distributed))\n",
        "    use_cuda = args.num_gpus > 0\n",
        "    logger.debug(\"Number of gpus available - {}\".format(args.num_gpus))\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if is_distributed:\n",
        "        logger.info('Initialize the distributed environment')\n",
        "        world_size = len(args.hosts)\n",
        "        os.environ['WORLD_SIZE'] = str(world_size)\n",
        "        host_rank = args.hosts.index(args.current_host)\n",
        "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\n",
        "        logger.info('Initialized the distributed environment:\\'{}\\' backend on {} nodes. '.format(\n",
        "            args.backend, dist.get_world_size()) + 'Current host rank is {}. Number of gpus: {}'.format(\n",
        "            dist.get_rank(), args.num_gpus))\n",
        "\n",
        "    # set the seed for generating random numbers\n",
        "    torch.manual_seed(args.seed)\n",
        "    if use_cuda:\n",
        "        logger.info('Use CUDA')\n",
        "        torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "    train_loader = _get_train_data_loader(args.batch_size, is_distributed, args.window_size, args.local, **kwargs)\n",
        "\n",
        "    logger.debug(\"Processes {}/{} ({:.0f}%) of train data\".format(\n",
        "        len(train_loader.sampler), len(train_loader.dataset),\n",
        "        100. * len(train_loader.sampler) / len(train_loader.dataset)\n",
        "    ))\n",
        "\n",
        "    model = Model(args.input_size, args.hidden_size, args.num_layers, args.num_classes).to(device)\n",
        "    if is_distributed and use_cuda:\n",
        "        logger.info('multi-machine multi-gpu case')\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model)\n",
        "    else:\n",
        "        logger.info('single-machine multi-gpu case or single-machine or multi-machine cpu case')\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for seq, label in train_loader:\n",
        "            seq = seq.clone().detach().view(-1, args.window_size, args.input_size).to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(seq)\n",
        "            loss = criterion(output, label.to(device))\n",
        "            loss.backward()\n",
        "            if is_distributed and not use_cuda:\n",
        "                # average gradients manually for multi-machine cpu case only\n",
        "                _average_gradients(model)\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        logger.debug('Epoch [{}/{}], Train_loss: {}'.format(\n",
        "            epoch, args.epochs, round(train_loss/len(train_loader.dataset), 4)\n",
        "        ))\n",
        "    logger.debug('Finished Training')\n",
        "    save_model(model, args.model_dir, args)\n",
        "\n",
        "\n",
        "def save_model(model, model_dir, args):\n",
        "    logger.info(\"Saving the model.\")\n",
        "    path = os.path.join(model_dir, 'model.pth')\n",
        "    torch.save(model.cpu().state_dict(), path)\n",
        "    # Save arguments used to create model for restoring the model later\n",
        "    model_info_path = os.path.join(model_dir, 'model_info.pth')\n",
        "    with open(model_info_path, 'wb') as f:\n",
        "        model_info = {\n",
        "            'input_size': args.input_size,\n",
        "            'hidden_size': args.hidden_size,\n",
        "            'num_layers': args.num_layers,\n",
        "            'num_classes': args.num_classes,\n",
        "            'num_candidates': args.num_candidates,\n",
        "            'window_size': args.window_size,\n",
        "        }\n",
        "        torch.save(model_info, f)\n",
        "\n",
        "\n",
        "def model_fn(model_dir):\n",
        "    logger.info('Loading the model.')\n",
        "    model_info = {}\n",
        "    with open(os.path.join(model_dir, 'model_info.pth'), 'rb') as f:\n",
        "        model_info = torch.load(f)\n",
        "    logger.debug('model_info: {}'.format(model_info))\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logger.info('Current device: {}'.format(device))\n",
        "    model = torch.nn.DataParallel(Model(input_size=model_info['input_size'],\n",
        "                                        hidden_size=model_info['hidden_size'],\n",
        "                                        num_layers=model_info['num_layers'],\n",
        "                                        num_classes=model_info['num_classes']))\n",
        "    with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:\n",
        "        model.load_state_dict(torch.load(f))\n",
        "    input_size = model_info['input_size']\n",
        "    window_size = model_info['window_size']\n",
        "    num_candidates = model_info['num_candidates']\n",
        "    return {'model': model.to(device),\n",
        "            'window_size': window_size,\n",
        "            'input_size': input_size,\n",
        "            'num_candidates': num_candidates}\n",
        "\n",
        "\n",
        "def input_fn(request_body, request_content_type):\n",
        "    logger.info('Deserializing the input data.')\n",
        "    if request_content_type == 'application/json':\n",
        "        input_data = json.loads(request_body)\n",
        "        return input_data\n",
        "    else:\n",
        "        raise ValueError(\"{} not supported by script!\".format(request_content_type))\n",
        "\n",
        "\n",
        "def predict_fn(input_data, model_info):\n",
        "    logger.info('Predict next template on this pattern series.')\n",
        "    line = input_data['line']\n",
        "    num_candidates = model_info['num_candidates']\n",
        "    input_size = model_info['input_size']\n",
        "    window_size = model_info['window_size']\n",
        "    model = model_info['model']\n",
        "\n",
        "    logger.info(line)\n",
        "    logger.debug(num_candidates)\n",
        "    logger.debug(input_size)\n",
        "    logger.debug(window_size)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logger.info('Current device: {}'.format(device))\n",
        "\n",
        "    predict_cnt = 0\n",
        "    anomaly_cnt = 0\n",
        "    predict_list = [0] * len(line)\n",
        "    for i in range(len(line) - window_size):\n",
        "        seq = line[i:i + window_size]\n",
        "        label = line[i + window_size]\n",
        "        seq = torch.tensor(seq, dtype=torch.float).view(-1, window_size, input_size).to(device)\n",
        "        label = torch.tensor(label).view(-1).to(device)\n",
        "        output = model(seq)\n",
        "        predict = torch.argsort(output, 1)[0][-num_candidates:]\n",
        "        if label not in predict:\n",
        "            anomaly_cnt += 1\n",
        "            predict_list[i + window_size] = 1\n",
        "        predict_cnt += 1\n",
        "    return {'anomaly_cnt': anomaly_cnt, 'predict_cnt': predict_cnt, 'predict_list': predict_list}\n",
        "\n",
        "\n",
        "def output_fn(prediction, accept):\n",
        "    logger.info('Serializing the generated output.')\n",
        "    if accept == \"application/json\":\n",
        "        return json.dumps(prediction), accept\n",
        "    raise ValueError(\"{} accept type is not supported by this script\".format(accept))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Data and model checkpoints directories\n",
        "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
        "                        help='input batch size for training (default: 64)')\n",
        "    parser.add_argument('--epochs', type=int, default=50, metavar='N',\n",
        "                        help='number of epochs to train (default: 50)')\n",
        "    parser.add_argument('--window-size', type=int, default=10, metavar='N',\n",
        "                        help='length of training window (default: 10)')\n",
        "    parser.add_argument('--input-size', type=int, default=1, metavar='N',\n",
        "                        help='model input size (default: 1)')\n",
        "    parser.add_argument('--hidden-size', type=int, default=64, metavar='N',\n",
        "                        help='hidden layer size (default: 64)')\n",
        "    parser.add_argument('--num-layers', type=int, default=2, metavar='N',\n",
        "                        help='number of model\\'s layer (default: 2)')\n",
        "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                        help='random seed (default: 1)')\n",
        "    parser.add_argument('--backend', type=str, default=None,\n",
        "                        help='backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)')\n",
        "\n",
        "    parser.add_argument('--num-classes', type=int, metavar='N',\n",
        "                        help='the number of model\\'s output, must same as pattern size!')\n",
        "    parser.add_argument('--num-candidates', type=int, metavar='N',\n",
        "                        help='the number of predictors sequences as correct predict.')\n",
        "\n",
        "    # Container environment\n",
        "    parser.add_argument('--hosts', type=list, default=json.loads(os.environ['SM_HOSTS']))\n",
        "    parser.add_argument('--current-host', type=str, default=os.environ['SM_CURRENT_HOST'])\n",
        "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
        "    parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n",
        "    parser.add_argument('--num-gpus', type=int, default=os.environ['SM_NUM_GPUS'])\n",
        "\n",
        "    # Local mode\n",
        "    parser.add_argument('--local', type=bool, default=False,\n",
        "                        help='local training model.')\n",
        "\n",
        "    train(parser.parse_args())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-bd885385e410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;31m# Container environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--hosts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SM_HOSTS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--current-host'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SM_CURRENT_HOST'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--model-dir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SM_MODEL_DIR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'SM_HOSTS'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1k3cP44PQrO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f619954-4b4b-4040-c615-01e84f0a7e7c"
      },
      "source": [
        "!pip install -q torch==1.5.1 torchvision\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 753.2MB 21kB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02MklD8oRbhp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d22f5e90-9a22-4a4d-d1d6-91741df17509"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}